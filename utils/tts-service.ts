import { v4 as uuidv4 } from 'uuid';
import WebSocket from 'ws';

// ç±»å‹å®šä¹‰
export type TTSTier = 'standard' | 'openai' | 'pro' | 'aliyun';

interface TTSRequest {
  text: string;
  tier: TTSTier;
  voice?: string; 
  reference_id?: string;
  returnSubtitles?: boolean;
  aliyunConfig?: {
    apiKey?: string; // DashScope/GreenNet é£æ ¼ API Keyï¼ˆæ¨èï¼‰
    akId?: string;   // é¢„ç•™ AK/SK BYOKï¼ˆå¦‚éœ€æ¢ Tokenï¼‰
    akSecret?: string;
    voiceId?: string; // å£°éŸ³å…‹éš†/å¤åˆ»éŸ³è‰² ID
  };
}

interface TTSResponse {
  audio: ArrayBuffer;
  srt?: string;
}

const FISH_AUDIO_API_URL = 'https://api.fish.audio/v1/tts';

// Edge TTS Constants
const EDGE_URL = 'wss://speech.platform.bing.com/consumer/speech/synthesize/readaloud/edge/v1?TrustedClientToken=6A5AA1D4EAFF4E9FB37E23D68491D6F4';
const EDGE_HEADERS = {
  'Pragma': 'no-cache',
  'Cache-Control': 'no-cache',
  'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.85 Safari/537.36 Edg/90.0.818.46',
  'Origin': 'chrome-extension://jdiccldimpdaibmpdkjnbmckianbfold',
  'Accept-Encoding': 'gzip, deflate, br',
  'Accept-Language': 'en-US,en;q=0.9',
};

// è¾…åŠ©ï¼šç”Ÿæˆ SRT æ—¶é—´æˆ³æ ¼å¼ (00:00:01,000)
function formatSRTTimeFromSeconds(totalSecondsFloat: number): string {
  const totalMs = Math.floor(totalSecondsFloat * 1000);
  const ms = totalMs % 1000;
  const totalSeconds = Math.floor(totalSecondsFloat);
  const seconds = totalSeconds % 60;
  const minutes = Math.floor(totalSeconds / 60) % 60;
  const hours = Math.floor(totalSeconds / 3600);

  const pad = (n: number, w: number) => n.toString().padStart(w, '0');
  return `${pad(hours, 2)}:${pad(minutes, 2)}:${pad(seconds, 2)},${pad(ms, 3)}`;
}

export async function generateAudio(request: TTSRequest): Promise<TTSResponse> {
  if (request.tier === 'standard') {
    try {
      let edgeVoice = 'en-US-AvaNeural'; 
      if (request.voice === 'echo' || request.voice === 'male') {
        edgeVoice = 'en-US-GuyNeural';
      }
      console.log(`Attempting Edge TTS with voice: ${edgeVoice}`);
      return await generateEdgeAudioPhysicalAlignment(request.text, edgeVoice);
    } catch (error) {
      console.error('Edge TTS failed, falling back to OpenAI:', error);
      const audio = await generateOpenAITTS(request.text, request.voice);
      return { audio };
    }
  } else if (request.tier === 'openai') {
    const audio = await generateOpenAITTS(request.text, request.voice);
    return { audio };
  } else if (request.tier === 'pro' && request.reference_id) {
    const audio = await generateFishAudio(request.text, request.reference_id);
    return { audio };
  } else if (request.tier === 'aliyun') {
    return await generateAliyunTTS(request.text, request.voice, request.aliyunConfig);
  } else {
    const audio = await generateOpenAITTS(request.text, request.voice);
    return { audio };
  }
}

async function generateEdgeAudioPhysicalAlignment(text: string, voice: string): Promise<TTSResponse> {
  const cleanText = text.replace(/[\r\n]+/g, ' ').replace(/\s+/g, ' ').trim();
  
  const subtitles: { text: string; startTime: number; endTime: number }[] = [];
  let currentSubtitle = { text: '', startTime: -1, endTime: 0 };

  const audioBuffer = await new Promise<ArrayBuffer>((resolve, reject) => {
    const ws = new WebSocket(EDGE_URL, { headers: EDGE_HEADERS });
    const requestId = uuidv4().replace(/-/g, '');
    const audioChunks: Buffer[] = [];

    ws.on('open', () => {
      const configMsg = `X-Timestamp:${new Date().toString()}\r\nContent-Type:application/json; charset=utf-8\r\nPath:speech.config\r\n\r\n` +
        JSON.stringify({
          context: {
            synthesis: {
              audio: {
                metadataoptions: {
                  sentenceBoundaryEnabled: "false", // æˆ‘ä»¬è‡ªå·±ç”¨ WordBoundary + æ ‡ç‚¹æ¥æ–­å¥
                  wordBoundaryEnabled: "true"
                },
                outputFormat: "audio-24khz-48kbitrate-mono-mp3"
              }
            }
          }
        });
      ws.send(configMsg);

      const ssml = `<speak version='1.0' xmlns='http://www.w3.org/2001/10/synthesis' xml:lang='en-US'><voice name='${voice}'><prosody pitch='+0Hz' rate='+0%' volume='+0%'>${cleanText}</prosody></voice></speak>`;
      const ssmlMsg = `X-RequestId:${requestId}\r\nContent-Type:application/ssml+xml\r\nX-Timestamp:${new Date().toString()}\r\nPath:ssml\r\n\r\n` + ssml;
      ws.send(ssmlMsg);
    });

    ws.on('message', (data, isBinary) => {
      if (isBinary) {
        const buffer = data as Buffer;
        const headerEnd = buffer.indexOf('\r\n\r\n');
        if (headerEnd !== -1) {
          const headers = buffer.subarray(0, headerEnd).toString();
          if (headers.includes('Path:audio')) {
            const audioData = buffer.subarray(headerEnd + 4);
            audioChunks.push(audioData);
          }
        }
      } else {
        const textData = data.toString();
        if (textData.includes('Path:audio.metadata')) {
          try {
            const jsonStart = textData.indexOf('\r\n\r\n');
            if (jsonStart !== -1) {
              const metadata = JSON.parse(textData.substring(jsonStart + 4));
              if (metadata.Metadata) {
                metadata.Metadata.forEach((meta: any) => {
                  if (meta.Type === 'WordBoundary') {
                    const wordText = meta.Data.text.Text;
                    const timestamp = meta.Data.Offset / 10000000; // ç§’
                    const duration = meta.Data.Duration / 10000000;
                    
                    if (currentSubtitle.startTime === -1) currentSubtitle.startTime = timestamp;
                    currentSubtitle.endTime = timestamp + duration;
                    
                    const prefix = currentSubtitle.text ? ' ' : '';
                    currentSubtitle.text += prefix + wordText;

                    // æ ‡ç‚¹æ–­å¥æ ¸å¿ƒé€»è¾‘ï¼š
                    // æ£€æŸ¥ wordText æ˜¯å¦ä»¥æ ‡ç‚¹ç»“å°¾
                    if (/[.?!,;ã€‚ï¼Ÿï¼ï¼Œï¼›]$/.test(wordText.trim())) {
                        subtitles.push({ ...currentSubtitle });
                        currentSubtitle = { text: '', startTime: -1, endTime: 0 };
                    }
                  }
                });
              }
            }
          } catch (e) { console.error('Metadata parsing error:', e); }
        }
        if (textData.includes('Path:turn.end')) {
            if (currentSubtitle.text.trim()) {
                subtitles.push({ ...currentSubtitle });
            }
            ws.close();
        }
      }
    });

    ws.on('close', (code, reason) => {
      if (audioChunks.length > 0) {
        const fullBuffer = Buffer.concat(audioChunks);
        resolve(fullBuffer.buffer.slice(fullBuffer.byteOffset, fullBuffer.byteOffset + fullBuffer.byteLength) as ArrayBuffer);
      } else {
         if (cleanText.length === 0) {
             resolve(new ArrayBuffer(0));
             return;
        }
        reject(new Error(`WebSocket closed without audio data. Code: ${code}, Reason: ${reason}`));
      }
    });

    ws.on('error', (err) => reject(err));
  });

  const srtOutput = subtitles.map((s, i) => {
    return `${i + 1}\n${formatSRTTimeFromSeconds(s.startTime)} --> ${formatSRTTimeFromSeconds(s.endTime)}\n${s.text.trim()}\n\n`;
  }).join('');

  console.log('FINAL SRT CONTENT:\n', srtOutput);
  return { audio: audioBuffer, srt: srtOutput };
}

// --- å…¶ä»– TTS å®ç° ---

async function generateFishAudio(text: string, referenceId?: string): Promise<ArrayBuffer> {
    const apiKey = process.env.FISH_AUDIO_API_KEY;
    if (!apiKey) {
      throw new Error('è¯·åœ¨é…ç½®ä¸­å¡«å†™ Fish Audio API Key');
    }
  
    const voiceId = referenceId || '7f9eb4a0378844a4805eb3c7d6c634c0'; 
  
    try {
      const response = await fetch(FISH_AUDIO_API_URL, {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${apiKey}`,
          'Content-Type': 'application/json', 
        },
        body: JSON.stringify({
          text: text,
          reference_id: voiceId,
          format: 'mp3',
          mp3_bitrate: 128,
        }),
      });
  
      if (!response.ok) {
        const errText = await response.text();
        throw new Error(`Fish Audio API Error: ${response.status} - ${errText}`);
      }
  
      return await response.arrayBuffer();
    } catch (error: any) {
      console.warn('Fish Audio failed:', error);
      if (process.env.OPENAI_API_KEY) {
         return await generateOpenAITTS(text);
      }
      throw error;
    }
}
  
async function generateOpenAITTS(text: string, voice: string = 'alloy'): Promise<ArrayBuffer> {
    const apiKey = process.env.OPENAI_API_KEY;
    if (!apiKey) throw new Error('Edge TTS å¤±è´¥ï¼Œä¸”æœªé…ç½® OpenAI API Keyã€‚');
  
    let openaiVoice = voice;
    if (voice === 'male') openaiVoice = 'echo';
    if (voice === 'female') openaiVoice = 'alloy';
    if (!['alloy', 'echo', 'fable', 'onyx', 'nova', 'shimmer'].includes(openaiVoice)) {
        openaiVoice = 'alloy';
    }

    const response = await fetch('https://api.openai.com/v1/audio/speech', {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${apiKey}`,
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        model: 'tts-1',
        input: text,
        voice: openaiVoice,
      }),
    });
  
    if (!response.ok) {
      throw new Error(`TTS API Error: ${response.statusText}`);
    }
  
    return await response.arrayBuffer();
}

// é˜¿é‡Œäº‘å†…ç½®å­—å¹•çš„æ–­å¥é€»è¾‘ï¼šå°†è¯è¯­ç»„åˆæˆå¥å­
function formatAliyunTimestampsToSRT(timestamps: any[]): string {
  if (!timestamps || timestamps.length === 0) return '';

  const segments: { text: string; start: number; end: number; wordCount: number }[] = [];
  let currentSegment = { text: '', start: -1, end: 0, wordCount: 0 };

  timestamps.forEach((item) => {
    const word = item.word || item.text || '';
    const start = item.start / 1000;
    const end = item.end / 1000;

    if (currentSegment.start === -1) currentSegment.start = start;

    currentSegment.text += word;
    currentSegment.end = end;
    currentSegment.wordCount++;

    // é‡æ ‡ç‚¹æˆ–è¯æ•°è¿‡å¤šåˆ™æ–­å¥
    if (/[ï¼Œã€‚ï¼Ÿï¼ï¼›,;?!]$/.test(word.trim()) || currentSegment.wordCount >= 12) {
      segments.push({ ...currentSegment });
      currentSegment = { text: '', start: -1, end: 0, wordCount: 0 };
    }
  });

  if (currentSegment.text) segments.push(currentSegment);

  return segments
    .map(
      (s, i) =>
        `${i + 1}\n${formatSRTTimeFromSeconds(s.start)} --> ${formatSRTTimeFromSeconds(s.end)}\n${s.text.trim()}\n\n`
    )
    .join('');
}

// --- Aliyun TTS (CosyVoice-v1) SSE æ¨¡å¼ ---
async function generateAliyunTTS(
  text: string,
  voice: string = 'cosyvoice',
  cfg?: { apiKey?: string; voiceId?: string; workspaceId?: string }
): Promise<TTSResponse> {
  const apiKey = cfg?.apiKey || process.env.ALIYUN_API_KEY;
  // é»˜è®¤éŸ³è‰²ï¼Œç¡®ä¿ voiceId æœ‰æ•ˆã€‚å¸¸è§å¦‚ï¼šlongxiaochun, longlaotie, longshuo ç­‰
  const voiceId = cfg?.voiceId || voice || 'longxiaochun'; 
  const workspaceId = cfg?.workspaceId || process.env.ALIYUN_WORKSPACE_ID;

  if (!apiKey) {
    throw new Error('é˜¿é‡Œäº‘ TTS éœ€è¦ API Key');
  }

  // æ ‡å‡† DashScope TTS æ¥å£
  const endpoint = 'https://dashscope.aliyuncs.com/api/v1/services/audio/tts/text-to-speech';

  // ä¿®æ­£çš„ Payload - ç§»é™¤å¯èƒ½å¯¼è‡´ 400 é”™è¯¯çš„å‚æ•°
  // ä¸»è¦ä¿®æ”¹ï¼šç§»é™¤äº† sample_rate å‚æ•°ï¼Œåªä¿ç•™å¿…éœ€çš„å‚æ•°
  const payload = {
    model: 'cosyvoice-v1', // æˆ–è€… 'cosyvoice-v2'
    input: {
      text: text
    },
    parameters: {
      voice: voiceId,
      format: 'mp3',
      // æ³¨é‡Šæ‰ sample_rateï¼Œä½¿ç”¨é»˜è®¤å€¼
      // sample_rate: 24000,  // è¿™ä¸ªå‚æ•°å¯èƒ½å¯¼è‡´ 400 é”™è¯¯
      enable_word_timestamp: true 
    }
  };

  console.log('å‘é€é˜¿é‡Œäº‘ TTS è¯·æ±‚:', JSON.stringify(payload, null, 2));

  const headers: Record<string, string> = {
    'Content-Type': 'application/json',
    'Authorization': `Bearer ${apiKey}`,
    // ğŸ’¡ å…³é”®ï¼šå¼€å¯ SSE æ¨¡å¼ï¼Œè¿™æ ·é˜¿é‡Œäº‘æ‰ä¼šè¿”å›åŒ…å« Metadata çš„æµ
    'X-DashScope-SSE': 'enable',
    ...(workspaceId && { 'X-DashScope-WorkSpace': workspaceId })
  };

  try {
    const res = await fetch(endpoint, {
      method: 'POST',
      headers,
      body: JSON.stringify(payload),
    });

    if (!res.ok) {
      let errorDetail = '';
      try {
        const errJson = await res.json();
        errorDetail = JSON.stringify(errJson, null, 2);
      } catch {
        errorDetail = await res.text();
      }
      
      console.error('é˜¿é‡Œäº‘ API è¯¦ç»†é”™è¯¯å“åº”:', {
        status: res.status,
        statusText: res.statusText,
        error: errorDetail
      });
      
      throw new Error(`Aliyun TTS Error: ${res.status} - ${errorDetail}`);
    }

    // --- å¤„ç† SSE æµæ•°æ® ---
    const reader = res.body?.getReader();
    if (!reader) throw new Error('æ— æ³•è¯»å–é˜¿é‡Œäº‘å“åº”æµ');

    let audioChunks: Buffer[] = [];
    let fullMetadata: any[] = [];
    const decoder = new TextDecoder();

    while (true) {
      const { done, value } = await reader.read();
      if (done) break;

      const chunkText = decoder.decode(value, { stream: true });
      const lines = chunkText.split('\n');

      for (const line of lines) {
        if (line.startsWith('data:')) {
          try {
            const jsonStr = line.replace('data:', '').trim();
            if (!jsonStr) continue;
            const data = JSON.parse(jsonStr);

            if (data.output?.audio_data) {
              audioChunks.push(Buffer.from(data.output.audio_data, 'base64'));
            }

            // æå–æ—¶é—´æˆ³ä¿¡æ¯
            if (data.output?.timestamps || data.output?.sentence_timestamps || data.output?.word_timestamps) {
               console.log('Detected timestamps in chunk:', JSON.stringify(data.output));
               fullMetadata = data.output.timestamps || data.output.sentence_timestamps || data.output.word_timestamps;
            }
          } catch (e) {
            // å¿½ç•¥ä¸å®Œæ•´ JSON
            console.warn('SSE æ•°æ®è§£æè­¦å‘Š:', e);
          }
        }
      }
    }

    // åˆå¹¶éŸ³é¢‘
    const fullAudioBuffer = Buffer.concat(audioChunks);
    const audioArrayBuffer = fullAudioBuffer.buffer.slice(
      fullAudioBuffer.byteOffset,
      fullAudioBuffer.byteOffset + fullAudioBuffer.byteLength
    ) as ArrayBuffer;

    // ç”Ÿæˆ SRTï¼ˆæŒ‰æ ‡ç‚¹/è¯æ•°æ–­å¥ï¼‰
    let srtOutput: string | undefined;
    if (fullMetadata.length > 0) {
      srtOutput = formatAliyunTimestampsToSRT(fullMetadata);
      console.log('é˜¿é‡Œäº‘å­—å¹•æ•°æ®:', fullMetadata);
      console.log('ç”Ÿæˆçš„å­—å¹•å†…å®¹:', srtOutput);
    } else {
      console.warn('é˜¿é‡Œäº‘ TTS æœªè¿”å›æ—¶é—´æˆ³æ•°æ®');
    }

    return { audio: audioArrayBuffer, srt: srtOutput };
    
  } catch (error: any) {
    console.error('é˜¿é‡Œäº‘ TTS è¯·æ±‚å¤±è´¥:', error);
    // ç”¨æˆ·è¦æ±‚ï¼šå¦‚æœæ˜¯é˜¿é‡Œäº‘å¤±è´¥ï¼Œç›´æ¥æŠ¥é”™ï¼Œä¸è¦å›é€€åˆ° OpenAI
    throw error;
  }
}